## Runs integration tests for the lexer and parser. Test fixtures are in the testdata directory.
## The fixtures are expected to have a simple format:
##
## # elixir
## elixir code...
##
## # tokens
## one token per line...
##
## # ast
## will return to this when I have a parser built...
##
## while there is nothing enforcing this structure I chose it over more "robust" formats for a
## a few reasons:
## - It's easy to parse in multiple languages. This has been a fun project and a good candidate
##   for use in learning other languages. The simple format makes them easy to use in virtually
##   any language and means every new implementation has a lot of tests to build from.
## - A full elixir parser isn't required. My first pass wrapped these in defmodules and defs.
##   While that is a lot more robust format it meant that I had to do a lot of tricks to have
##   useable tests when I only have a lexer and no parser built.
## - I can write all of the tests with syntax highlighting and an elixir formatter. This is
##   a nice creature comfort as opposed to dealing with lots of multiline strings everywhere.
module []

import Lexer

# test fixtures. These are probably better read at runtime, but this is
# the convenient path for now.
import "testdata/if.exs" as if_test : Str
import "testdata/with.exs" as with_test : Str
import "testdata/with_guards.exs" as with_guards_test : Str
import "testdata/with_else.exs" as with_else_test : Str

# any test fixture that we want to test needs to be added to this list.
test_files =
    Dict.empty({})
    |> Dict.insert("if.exs", if_test)
    |> Dict.insert("with.exs", with_test)
    |> Dict.insert("with_guards.exs", with_guards_test)
    |> Dict.insert("with_else.exs", with_else_test)

FixtureParserState : [None, ParsingElixir, ParsingTokens, ParsingAST]
TestData : { filename : Str, elixir : Str, str_tokens : List Str, ast : Str, parser_state : FixtureParserState }

# parse the text fixture into its 3 sections, the original elixir code, the tokens we expect
# and the ast we expect.
parse_fixture : Str, Str -> TestData
parse_fixture = |filename, input|
    Str.split_on(input, "\n")
    |> List.walk(
        { filename: filename, elixir: "", str_tokens: [], ast: "", parser_state: None },
        |state, line|
            trimmed_line = Str.trim(line)

            parser_state =
                when trimmed_line is
                    "# elixir" -> ParsingElixir
                    "# tokens" -> ParsingTokens
                    "# ast" -> ParsingAST
                    _ -> state.parser_state

            if is_usable_line(trimmed_line) then
                when parser_state is
                    None -> state
                    ParsingElixir -> { state & elixir: Str.concat(state.elixir, "${line}\n"), parser_state: parser_state }
                    ParsingTokens -> { state & str_tokens: List.append(state.str_tokens, trimmed_line), parser_state: parser_state }
                    ParsingAST -> state
            else
                { state & parser_state: parser_state },
    )

# when parsing the line we want to throw out the comments and empty lines
is_usable_line : Str -> Bool
is_usable_line = |line|
    line != "" and !Str.starts_with(line, "#")

# compare the tokens generated by the lexer with the tokens in the test fixture
test_lexer = |test_data|
    tokens = Lexer.process(test_data.elixir)
    str_tokens = List.map(tokens, token_to_elixir_term)
    expect str_tokens == test_data.str_tokens
    Ok

# takes a token and returns an elixir representation of the token where a token with no payload
# is simple `:TokenName` and a token with a payload is an elixir tuple `{:TokenName, payload}`
token_to_elixir_term : Lexer.Token -> Str
token_to_elixir_term = |token|
    when token is
        TokenIdentifier x -> "{:TokenIdentifier, \"${x}\"}"
        TokenNumber x -> "{:TokenNumber, \"${x}\"}"
        TokenString x -> "{:TokenString, \"${x}\"}"
        TokenAtom x -> "{:TokenAtom, \"${x}\"}"
        TokenIllegal _x -> "{:TokenIllegal}" # this value isn't important to the tests
        _ -> ":${Inspect.to_str(token)}"

# top level expect just used to run all of the inline expects
expect
    Dict.walk(
        test_files,
        Bool.true,
        |_state, filename, contents|
            test_data = parse_fixture(filename, contents)
            # this when isn't important, but keeps the compiler from warning about
            # not assigning it, but also warning when assigning it to just _
            when test_lexer(test_data) is
                Ok -> Bool.true,
    )
    == Bool.true
